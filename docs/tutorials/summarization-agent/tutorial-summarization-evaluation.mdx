---
id: tutorial-summarization-evaluation
title: Evaluation
sidebar_label: Evaluation
---

In the previous section, we built a meeting summarization agent and reviewed the output it generated from a sample conversation transcript.
However, the problem we've faced is assessing the quality of our summarization agent. Many developers tend to eyeball the results of their LLM applications, this is a common and significant issue in LLM application development.

In this section we are going to see how to evaluate our `MeetingSummarizer` using **DeepEval**, a powerful open-source LLM evaluation framework.

## Defining Evaluation Criteria

Defining evaluation criteria is arguably the most important part of assessing an LLM application's performance. LLM applications are always made with a clear goal in mind, and the evaluation criteria must be defined by taking this goal into consideration. 
Let's look at the goal we've defined for our meeting summarization agent:

The agent we've defined must process meeting-style transcripts and generate:

- A brief summary of the discussion.
- A list of action items.

Our evaluation criteria are directly based on the two goals mentioned above. Our summarization agent is designed to improve team productivity, and hence the **summaries that are generated must be concise**.
Another key criterion to consider is that the **action items generated must be correct and cover all the key actions** that were mentioned in the meeting. 

### Selecting the metrics

As mentioned above, the criteria we've defined are:

- Generated summaries must be concise
- Generated action items must be correct and cover all key actions. 

These two criteria are specific to our `MeetingSummarizer`, but your use case might differ â€” and that's perfectly fine. **DeepEval** supports custom evaluation through the [`GEval`](https://deepeval.com/docs/metrics-llm-evals) metric, which we'll use here. And this metric can be used for any custom criteria we will define. [Here's a great article explaining how G-Eval works](https://www.confident-ai.com/blog/g-eval-the-definitive-guide).
DeepEval also provides a [`SummarizationMetric`](https://deepeval.com/docs/metrics-summarization) specifically designed for summarization tasks, [here's a great article](https://www.confident-ai.com/blog/a-step-by-step-guide-to-evaluating-an-llm-text-summarization-task) on how this metric was developed. And so we are going to use this metric as well.

## Evaluation

Here's how we can define our metrics and evaluate our `MeetingSummarizer`:

```python title="test_summarizer.py"
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import SummarizationMetric, GEval
from deepeval import evaluate

testcase = LLMTestCase(
    input=transcript, # your full meeting transcript as a string
    actual_output=summary # provide the summary generated by your summarizer here
)

metrics = [
    SummarizationMetric(),
    GEval(
        name="Concision",
        criteria="Assess whether the summary is concise and focused only on the essential points of the meeting? It should avoid repetition, irrelevant details, and unnecessary elaboration.",
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
    ),
    GEval(
        name="Action Item Accuracy",
        criteria="Are the action items accurate, complete, and clearly reflect the key tasks or follow-ups mentioned in the meeting?",
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
    )
]

evaluate(test_cases=[testcase], metrics=metrics)
```

You can save the above code in a test file named `test_summarizer.py` and run the following code in your terminal to evaluate your summarizer:

```bash
deepeval test run test_summarizer.py
```

:::tip
It is highly recommended that you use [**Confident AI**](https://www.confident-ai.com), deepeval's cloud platform that allows you to view your test results in a much more intuitive way. Here's how you can [set up Confident AI](https://deepeval.com/tutorials/tutorial-setup#setting-up-confident-ai). Or you can simply run the following code in the terminal to set it up yourself:
```bash
deepeval login
```
**It's free to get started!** _(No credit card required.)_
:::

And that's all it takes to evaluate your summarization agent using `deepeval`. In the next section we are going to see [how to improve your summarization agent](tutorial-summarization-improvement) with the help of deepeval's evaluation results.